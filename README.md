# Explain Further: Multi-Level Explanations for Fake News Detection Using Large Language Models

## Abstract

The exponential rise in the dissemination of fake news in recent years has underscored the necessity for automated systems capable of detecting such content. Various techniques for fake news detection have demonstrated promising outcomes. Nevertheless, the modelâ€™s high performance is no longer enough, the explainability of the system's decisions is equally pivotal. Although numerous scholars have addressed the challenge of explainability, their solutions often assume a user base with the technical expertise to comprehend these explanations. Hence, this paper aims to introduce a novel **Multi-Level Model-Agnostic Post-Hoc Explanations (MAPE)** framework, which is designed to enhance interpretability in the context of fake news detection. **MAPE** confronts the difficulty of elucidating complex model predictions by providing multi-tiered explanations, allowing users to select the level of detail that suits their needs. The framework utilizes explainable Artificial Intelligence (AI) techniques and Large Language Models (LLMs) to produce explanations that vary from basic feature highlights to detailed, context-rich narratives. Evaluation results from both human evaluators and LLMs suggest that more comprehensive explanations lead to improved user perceptions of clarity and persuasiveness, with detailed explanations surpassing technical descriptions. Furthermore, assessments by LLMs are closely aligned with human evaluations across most quality dimensions.

## Repository Structure

- **`lime_explanation_feature_generation.py`**  
  Uses LIME to generate feature-based explanations for a BERT-based fake news detection model. This script wraps a BERT model to predict labels and produces LIME explanations that serve as the basis for further narrative explanation generation.

- **`llm_explanation_generation.py`**  
  Generates multi-level explanations for fake news detection using GPT-3.5 and GPT-4.  
  - **Level 2:** Transforms LIME feature contribution into simple narrative explanations.  
  - **Level 3:** Provides step-by-step reasoning for why a news item is classified as Fake or Real.
    
- **`llm_evaluation.py`**  
  Processes input CSV files containing explanations and evaluates their quality using an OpenAI LLM. It parses LLM responses to extract scores and explanations for each metric.

- **`results_analysis_llm.py`**  
  Analyzes evaluation responses generated by LLMs by computing summary statistics (mean scores) for quality criteria such as Fluency, Informativeness, Persuasiveness, and Soundness.

- **`results_analysis_human.py`**  
  Processes human evaluation data to normalize and split the data by explanation type, computes quality metrics for both fake and real news, conducts paired t-tests, and performs misclassification analysis.



